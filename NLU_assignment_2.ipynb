{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLU-assignment-2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoZanella/NLU-assignement-2/blob/main/NLU_assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O99OsUUONTEK"
      },
      "source": [
        "# NLU assignment n.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRkD3V65KQQZ"
      },
      "source": [
        "Update SpaCy to version 3 and download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvecUgE53Qqj"
      },
      "source": [
        "%%capture\n",
        "!pip install --upgrade spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!wget -nc https://raw.githubusercontent.com/esrel/NLU.Lab.2021/master/src/conll.py\n",
        "!wget -nc https://github.com/esrel/NLU.Lab.2021/raw/master/src/conll2003.zip\n",
        "!unzip -n conll2003.zip -d conll2003"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEGnfiiQKacq"
      },
      "source": [
        "Load the dataset and Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJVn2Qwsv6Vu"
      },
      "source": [
        "# Imports\n",
        "import random\n",
        "import conll\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from spacy.tokens import Token\n",
        "from spacy.training import Alignment\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "gt = conll.read_corpus_conll(\"./conll2003/train.txt\", fs=\" \")\n",
        "gt.extend(conll.read_corpus_conll(\"./conll2003/test.txt\", fs=\" \"))\n",
        "gt.extend(conll.read_corpus_conll(\"./conll2003/dev.txt\", fs=\" \"))\n",
        "\n",
        "# Removing reference lines\n",
        "gt = [tag_sent for tag_sent in gt if tag_sent[0][0] != '-DOCSTART-']\n",
        "\n",
        "# Limit the dataset, for a faster analysis in the following code\n",
        "gt = random.sample(gt, 8000)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kDDUX8DPF_5"
      },
      "source": [
        "## Task 1: SpaCy NER evaluation\n",
        "Evaluate spaCy NER on CoNLL 2003 data (provided)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oA4UkLBrJvm"
      },
      "source": [
        "### Part 1.1: SpaCy NER and alignement\n",
        "As first step, we need to evaluate the sentences with SpaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13PCnS7ODoqV"
      },
      "source": [
        "Creation of custom extentions to save the dataset information directly in the SpaCy tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjkNY2ZbNvMg"
      },
      "source": [
        "Token.set_extension(\"ent_ref\", default='')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1UvP9ygYDAY"
      },
      "source": [
        "Translation function: By scanning the entire Conll dataset, you can see that the only Entities present are:\n",
        "`'LOC', 'ORG', 'PER', 'MISC'`\n",
        "\n",
        "SpaCy Entities, more detailed, should be translated according to their meaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDFcsnpNVXMC"
      },
      "source": [
        "def to_ref_entity(token):\n",
        "  ent_iob = token.ent_iob_\n",
        "  ent_type = token.ent_type_\n",
        "  if ent_type == 'ORG' : # Organizations\n",
        "    ent_type = 'ORG'\n",
        "  elif ent_type == 'PERSON':  # Persons\n",
        "    ent_type = 'PER'\n",
        "  elif ent_type == 'GPE' or ent_type == 'FAC' or ent_type == 'LOC':  # Localities\n",
        "    ent_type = 'LOC'\n",
        "  else:\n",
        "    ent_type = 'MISC'\n",
        "  \n",
        "  if ent_iob == 'O':\n",
        "    return ent_iob \n",
        "  else:\n",
        "    return f\"{ent_iob}-{ent_type}\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfusohC_YeGl"
      },
      "source": [
        "The spacy tokenization is different from the one provided in the dataset.\n",
        "I checked `alignment.x2y.lengths` and verified that spacy tokens needs to be merged at most, never to be splitted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbfFVtukdkrO"
      },
      "source": [
        "docs = []\n",
        "for gt_sentence in gt:\n",
        "  # List of ground truth tokens (token, POS, chunk, entity)\n",
        "  gt_tokens = [tup[0] for tup in gt_sentence]\n",
        "  # Create Doc object and extract tokens\n",
        "  doc = nlp(\" \".join(gt_tokens))\n",
        "  doc_tokens = [t.text for t in doc]\n",
        "  \n",
        "  # Get the alignment: .y2x.lengths has the merge informations\n",
        "  # .x2y.lengths is all ones with the tokenization considered\n",
        "  alignment = Alignment.from_strings(doc_tokens, gt_tokens)\n",
        "  # Merge together tokens to reflect ground truth tokenization\n",
        "  with doc.retokenize() as retokenizer:\n",
        "    doc_idx = 0\n",
        "    for length in alignment.y2x.lengths:\n",
        "      if length > 1:\n",
        "        retokenizer.merge(doc[doc_idx:doc_idx+length])\n",
        "      doc_idx += length\n",
        "\n",
        "  # Add the information about chunk division and entity\n",
        "  for token, ref in zip(doc, gt_sentence):\n",
        "    token._.ent_ref = ref[3]\n",
        "  docs.append(doc)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEpF0UVWNJ-c"
      },
      "source": [
        "### Part 1.2: Token-level performance\n",
        "Report token-level performance (per class and total)\n",
        "  - accuracy of correctly recognizing all tokens that belong to named entities (i.e. tag-level accuracy)\n",
        "  - to get per-class and total token-level performances you use scikit-learn's classification report, like we did in the lab on evaluation (you don't need to compute accuracy per-class, such thing does not exist)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3ga2XpUi6bo"
      },
      "source": [
        "def token_entities(docs):\n",
        "  \"\"\"Extract token-level predicted and reference Named Entities, as requested by classification_report\"\"\"\n",
        "  token_NE_ref = []\n",
        "  token_NE_pred = []\n",
        "\n",
        "  for doc in docs:\n",
        "    for token in doc:\n",
        "      token_NE_ref.append(token._.ent_ref)\n",
        "      token_NE_pred.append(to_ref_entity(token))\n",
        "  return token_NE_ref, token_NE_pred"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "tMXHHB6efcmv",
        "outputId": "dcd094cb-59a8-403e-c392-7bd5c348312a"
      },
      "source": [
        "# Print the results\n",
        "NE_ref, NE_pred = token_entities(docs)\n",
        "print(classification_report(NE_ref, NE_pred))\n",
        "print('='*80)\n",
        "# Optional Confusion Matrix\n",
        "y_actu = pd.Series(NE_ref, name='Actual')\n",
        "y_pred = pd.Series(NE_pred, name='Predicted')\n",
        "pd_tbl = pd.crosstab(y_pred, y_actu)\n",
        "pd_tbl.round(decimals=3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.79      0.69      0.74      4025\n",
            "      B-MISC       0.12      0.57      0.20      1993\n",
            "       B-ORG       0.48      0.31      0.37      3588\n",
            "       B-PER       0.80      0.66      0.72      3920\n",
            "       I-LOC       0.55      0.59      0.57       651\n",
            "      I-MISC       0.06      0.31      0.09       722\n",
            "       I-ORG       0.45      0.54      0.49      1971\n",
            "       I-PER       0.82      0.82      0.82      2670\n",
            "           O       0.95      0.87      0.91     97195\n",
            "\n",
            "    accuracy                           0.82    116735\n",
            "   macro avg       0.56      0.60      0.55    116735\n",
            "weighted avg       0.89      0.82      0.85    116735\n",
            "\n",
            "================================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Actual</th>\n",
              "      <th>B-LOC</th>\n",
              "      <th>B-MISC</th>\n",
              "      <th>B-ORG</th>\n",
              "      <th>B-PER</th>\n",
              "      <th>I-LOC</th>\n",
              "      <th>I-MISC</th>\n",
              "      <th>I-ORG</th>\n",
              "      <th>I-PER</th>\n",
              "      <th>O</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Predicted</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>B-LOC</th>\n",
              "      <td>2787</td>\n",
              "      <td>50</td>\n",
              "      <td>406</td>\n",
              "      <td>120</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>17</td>\n",
              "      <td>6</td>\n",
              "      <td>113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B-MISC</th>\n",
              "      <td>84</td>\n",
              "      <td>1142</td>\n",
              "      <td>72</td>\n",
              "      <td>53</td>\n",
              "      <td>0</td>\n",
              "      <td>33</td>\n",
              "      <td>22</td>\n",
              "      <td>2</td>\n",
              "      <td>7880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B-ORG</th>\n",
              "      <td>187</td>\n",
              "      <td>194</td>\n",
              "      <td>1102</td>\n",
              "      <td>326</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>22</td>\n",
              "      <td>4</td>\n",
              "      <td>457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B-PER</th>\n",
              "      <td>94</td>\n",
              "      <td>47</td>\n",
              "      <td>357</td>\n",
              "      <td>2588</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>15</td>\n",
              "      <td>40</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-LOC</th>\n",
              "      <td>78</td>\n",
              "      <td>18</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>383</td>\n",
              "      <td>21</td>\n",
              "      <td>94</td>\n",
              "      <td>30</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-MISC</th>\n",
              "      <td>3</td>\n",
              "      <td>66</td>\n",
              "      <td>21</td>\n",
              "      <td>16</td>\n",
              "      <td>19</td>\n",
              "      <td>221</td>\n",
              "      <td>23</td>\n",
              "      <td>21</td>\n",
              "      <td>3552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-ORG</th>\n",
              "      <td>64</td>\n",
              "      <td>66</td>\n",
              "      <td>254</td>\n",
              "      <td>40</td>\n",
              "      <td>92</td>\n",
              "      <td>156</td>\n",
              "      <td>1071</td>\n",
              "      <td>131</td>\n",
              "      <td>501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-PER</th>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "      <td>43</td>\n",
              "      <td>20</td>\n",
              "      <td>23</td>\n",
              "      <td>217</td>\n",
              "      <td>2184</td>\n",
              "      <td>155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>O</th>\n",
              "      <td>719</td>\n",
              "      <td>407</td>\n",
              "      <td>1349</td>\n",
              "      <td>729</td>\n",
              "      <td>125</td>\n",
              "      <td>246</td>\n",
              "      <td>490</td>\n",
              "      <td>252</td>\n",
              "      <td>84399</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Actual     B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
              "Predicted                                                                 \n",
              "B-LOC       2787      50    406    120      7       8     17      6    113\n",
              "B-MISC        84    1142     72     53      0      33     22      2   7880\n",
              "B-ORG        187     194   1102    326      2       8     22      4    457\n",
              "B-PER         94      47    357   2588      3       6     15     40     82\n",
              "I-LOC         78      18     12      5    383      21     94     30     56\n",
              "I-MISC         3      66     21     16     19     221     23     21   3552\n",
              "I-ORG         64      66    254     40     92     156   1071    131    501\n",
              "I-PER          9       3     15     43     20      23    217   2184    155\n",
              "O            719     407   1349    729    125     246    490    252  84399"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCAHh9aBdHJr"
      },
      "source": [
        "### Part 1.3: Chunk-level performance\n",
        "Report CoNLL chunk-level performance (per class and total):\n",
        "  - Precision, recall, f-measure of correctly recognizing all the named entities in a chunk per class and total.\n",
        "  - To get chunk-level NER performance, you simply need to use conll.py's evaluate, that computes segmentation and labeling performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnDcyVvOaSq6"
      },
      "source": [
        "def chunk_entities(docs):\n",
        "  \"\"\"Transform tokens into (text, iob), as requested by conll.evaluate()\"\"\"\n",
        "  chunk_NE_ref = []\n",
        "  chunk_NE_pred = []\n",
        "  \n",
        "  for doc in docs:\n",
        "    chunk_NE_pred.append([(t.text, to_ref_entity(t)) for t in doc])\n",
        "    chunk_NE_ref.append([(t.text, t._.ent_ref) for t in doc])\n",
        "  \n",
        "  return chunk_NE_ref, chunk_NE_pred"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "LQTFrPuLScyF",
        "outputId": "4a10b727-ae14-4840-90d9-9dc6f80c2469"
      },
      "source": [
        "# Print the results\n",
        "NE_ref, NE_pred = chunk_entities(docs)\n",
        "results = conll.evaluate(NE_ref, NE_pred)\n",
        "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
        "pd_tbl.round(decimals=3)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>p</th>\n",
              "      <th>r</th>\n",
              "      <th>f</th>\n",
              "      <th>s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>LOC</th>\n",
              "      <td>0.781</td>\n",
              "      <td>0.681</td>\n",
              "      <td>0.728</td>\n",
              "      <td>4025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MISC</th>\n",
              "      <td>0.116</td>\n",
              "      <td>0.540</td>\n",
              "      <td>0.191</td>\n",
              "      <td>1993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ORG</th>\n",
              "      <td>0.429</td>\n",
              "      <td>0.275</td>\n",
              "      <td>0.335</td>\n",
              "      <td>3588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PER</th>\n",
              "      <td>0.778</td>\n",
              "      <td>0.642</td>\n",
              "      <td>0.703</td>\n",
              "      <td>3920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>total</th>\n",
              "      <td>0.399</td>\n",
              "      <td>0.541</td>\n",
              "      <td>0.460</td>\n",
              "      <td>13526</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           p      r      f      s\n",
              "LOC    0.781  0.681  0.728   4025\n",
              "MISC   0.116  0.540  0.191   1993\n",
              "ORG    0.429  0.275  0.335   3588\n",
              "PER    0.778  0.642  0.703   3920\n",
              "total  0.399  0.541  0.460  13526"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH-y2WbUPZu5"
      },
      "source": [
        "## Task 2: Grouping of Entities\n",
        "Write a function to group recognized named entities using noun_chunks method of [spaCy](https://spacy.io/usage/linguistic-features#noun-chunks)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoXv9m1G5j9q"
      },
      "source": [
        "def grouped_entities(sentence):\n",
        "  doc = nlp(sentence)\n",
        "  n_chunks = list(doc.noun_chunks)\n",
        "  entities = []\n",
        "  \n",
        "  curr_chunk = 0\n",
        "  curr_token = 0\n",
        "  chunk_ents = set()\n",
        "  for token in doc:\n",
        "    if curr_chunk < len(n_chunks) and token == n_chunks[curr_chunk][curr_token]:  # Token is next in the noun chunk\n",
        "      if token.ent_type_ != '': # Middle or final token\n",
        "        chunk_ents.add(token.ent_type_)\n",
        "      curr_token += 1\n",
        "      if token == n_chunks[curr_chunk][-1]:  # Last token of the current chunk\n",
        "        if len(chunk_ents) > 0:\n",
        "          entities.append(sorted(chunk_ents))  # Sorted, so it's a list without notion of token ordering\n",
        "        curr_chunk += 1  # Look for next token\n",
        "        curr_token = 0  # At the first position\n",
        "        chunk_ents = set()  # Reset the set of chunk's tokens\n",
        "    elif token.ent_type_ != '':\n",
        "      entities.append([token.ent_type_])\n",
        "  return entities"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6JyWC2CAc76",
        "outputId": "e1d649e5-b86a-4509-c24d-ae5bbb5921e0"
      },
      "source": [
        "# Testing the function\n",
        "grouped_entities(\"Apple's Steve Jobs died in 2011 in Palo Alto, California.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['ORG', 'PERSON'], ['DATE'], ['GPE'], ['GPE']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVe_vI8Y5Q-8"
      },
      "source": [
        "### Part 2.1: Frequency analysis\n",
        "Analyze the groups in terms of most frequent combinations (i.e. NER types that go together)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oaIaTn9E493"
      },
      "source": [
        "From the ground truth dataset, extract the corpus with all plain text sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj8oolMdEpRo"
      },
      "source": [
        "corpus = [\" \".join([tup[0] for tup in gt_sentence]) for gt_sentence in gt]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wwFAQUSFUAk"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "frequencies = Counter()\n",
        "for sentence in corpus: \n",
        "  entities = grouped_entities(sentence)\n",
        "  for group in entities:\n",
        "    combination = '-'.join(group)\n",
        "    frequencies[combination] += 1"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQwW55uefzHt",
        "outputId": "f4e26c50-21eb-49d8-9c9c-7f00e2678bd0"
      },
      "source": [
        "for combination, counter in frequencies.most_common():\n",
        "  print(f\"{combination}: {counter}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DATE: 4865\n",
            "CARDINAL: 3901\n",
            "GPE: 3079\n",
            "PERSON: 3052\n",
            "ORG: 2417\n",
            "NORP: 864\n",
            "MONEY: 547\n",
            "ORDINAL: 422\n",
            "TIME: 411\n",
            "QUANTITY: 222\n",
            "PERCENT: 209\n",
            "LOC: 116\n",
            "EVENT: 107\n",
            "NORP-PERSON: 106\n",
            "FAC: 94\n",
            "WORK_OF_ART: 86\n",
            "CARDINAL-PERSON: 79\n",
            "GPE-PERSON: 71\n",
            "PRODUCT: 67\n",
            "CARDINAL-ORG: 67\n",
            "ORG-PERSON: 64\n",
            "GPE-ORG: 49\n",
            "CARDINAL-NORP: 38\n",
            "LAW: 38\n",
            "DATE-ORG: 32\n",
            "DATE-PERSON: 29\n",
            "DATE-TIME: 25\n",
            "NORP-ORG: 24\n",
            "DATE-GPE: 23\n",
            "CARDINAL-GPE: 23\n",
            "NORP-ORDINAL: 20\n",
            "LANGUAGE: 18\n",
            "DATE-NORP: 17\n",
            "GPE-NORP: 17\n",
            "DATE-EVENT: 15\n",
            "ORDINAL-PERSON: 15\n",
            "GPE-ORDINAL: 15\n",
            "CARDINAL-DATE: 12\n",
            "ORDINAL-ORG: 10\n",
            "ORG-PRODUCT: 8\n",
            "GPE-PRODUCT: 8\n",
            "CARDINAL-ORDINAL: 7\n",
            "LANGUAGE-ORDINAL: 6\n",
            "MONEY-ORG: 5\n",
            "ORDINAL-QUANTITY: 5\n",
            "DATE-ORDINAL: 4\n",
            "EVENT-PERSON: 4\n",
            "EVENT-NORP: 4\n",
            "CARDINAL-PRODUCT: 4\n",
            "FAC-GPE: 4\n",
            "PERSON-TIME: 3\n",
            "DATE-WORK_OF_ART: 3\n",
            "DATE-NORP-PERSON: 3\n",
            "CARDINAL-EVENT: 3\n",
            "EVENT-ORDINAL: 3\n",
            "DATE-MONEY: 3\n",
            "CARDINAL-PERCENT: 3\n",
            "DATE-PERCENT: 3\n",
            "LOC-NORP: 2\n",
            "PERSON-WORK_OF_ART: 2\n",
            "CARDINAL-GPE-PERSON: 2\n",
            "GPE-MONEY: 2\n",
            "LOC-PERSON: 2\n",
            "GPE-ORG-PERSON: 2\n",
            "DATE-QUANTITY: 2\n",
            "GPE-PERCENT: 2\n",
            "GPE-TIME: 2\n",
            "CARDINAL-LOC: 2\n",
            "CARDINAL-QUANTITY: 2\n",
            "EVENT-GPE: 2\n",
            "DATE-PRODUCT: 2\n",
            "GPE-LOC: 1\n",
            "CARDINAL-ORDINAL-ORG: 1\n",
            "CARDINAL-MONEY-ORG: 1\n",
            "DATE-NORP-ORG: 1\n",
            "GPE-NORP-ORDINAL: 1\n",
            "CARDINAL-GPE-ORG: 1\n",
            "CARDINAL-DATE-GPE-ORG: 1\n",
            "FAC-PERSON: 1\n",
            "MONEY-PERSON: 1\n",
            "CARDINAL-GPE-LOC-ORG: 1\n",
            "LAW-ORG-PERSON: 1\n",
            "ORDINAL-TIME: 1\n",
            "DATE-LANGUAGE: 1\n",
            "GPE-LOC-PERSON: 1\n",
            "ORG-PERCENT: 1\n",
            "CARDINAL-DATE-GPE: 1\n",
            "CARDINAL-ORG-PERSON: 1\n",
            "GPE-NORP-ORG: 1\n",
            "DATE-FAC: 1\n",
            "NORP-ORG-PERSON: 1\n",
            "GPE-PERSON-TIME: 1\n",
            "PERSON-QUANTITY: 1\n",
            "GPE-LOC-ORG: 1\n",
            "GPE-LANGUAGE: 1\n",
            "LANGUAGE-ORG: 1\n",
            "MONEY-ORG-PRODUCT: 1\n",
            "FAC-ORG: 1\n",
            "GPE-WORK_OF_ART: 1\n",
            "DATE-GPE-ORG: 1\n",
            "DATE-GPE-NORP-PERSON: 1\n",
            "LANGUAGE-ORDINAL-ORG: 1\n",
            "CARDINAL-FAC: 1\n",
            "LOC-ORDINAL: 1\n",
            "CARDINAL-GPE-NORP: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfn7btkXPfSs"
      },
      "source": [
        "## Task 3: Covering full noun-compounds\n",
        "One of the possible post-processing steps is to fix segmentation errors. Write a function that extends the entity span to cover the full noun-compounds. Make use of compound dependency relation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX_5zda-tWyd"
      },
      "source": [
        "### Part 3.1: Definition and application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ9uvCQDneGk"
      },
      "source": [
        "You have to be careful when extending entities with the coumpound, because you could overwrite other entities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-1HNIHrju0X"
      },
      "source": [
        "from spacy.tokens import Span\n",
        "\n",
        "def expand_entities(doc):\n",
        "  entities = []\n",
        "  for ents_i, ent in enumerate(doc.ents):\n",
        "    ent_start = ent.start\n",
        "    ent_end = ent.end\n",
        "    # List of all the children of the entity span tokens\n",
        "    subtree = list(ent.root.subtree) \n",
        "    search_start = subtree[0].i\n",
        "    search_end = subtree[-1].i + 1\n",
        "    # The search should be limited by previous and next entities\n",
        "    if ents_i > 0 and doc.ents[ents_i - 1].end > search_start:\n",
        "      search_start = doc.ents[ents_i - 1].end\n",
        "    if ents_i < (len(doc.ents) - 1) and doc.ents[ents_i + 1].start < search_end:\n",
        "      search_end = doc.ents[ents_i + 1].start\n",
        "    # Extend the head\n",
        "    token = doc[search_start]\n",
        "    while token.i < ent_start:\n",
        "      compound_root = token\n",
        "      while compound_root.dep_ == 'compound' and not (ent_start <= compound_root.i < ent_end):\n",
        "        compound_root = compound_root.head\n",
        "      if ent_start <= compound_root.i < ent_end:\n",
        "        ent_start = token.i\n",
        "      token = token.nbor()\n",
        "    # Extend the tail\n",
        "    token = doc[search_end - 1]\n",
        "    while token.i >= ent_end:\n",
        "      compound_root = token\n",
        "      while compound_root.dep_ == 'compound' and not (ent_start <= compound_root.i < ent_end):\n",
        "        compound_root = compound_root.head\n",
        "      if ent_start <= compound_root.i < ent_end:\n",
        "        ent_end = token.i + 1\n",
        "      token = token.nbor(-1)\n",
        "    # Add the expanded entity to the list\n",
        "    entity = Span(doc, ent_start, ent_end, label=ent.label_)\n",
        "    entities.append(entity)\n",
        "  # Set the extended entities\n",
        "  doc.set_ents(entities)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tpKTz-CB-t7"
      },
      "source": [
        "We can directy apply the postprocessing to the docs object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EudFK_mzJ_1s"
      },
      "source": [
        "# Application of the post-processing step\n",
        "for doc in docs:\n",
        "  expand_entities(doc)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSY01zuCtf_F"
      },
      "source": [
        "### Part 3.2: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSXVOvA4CImE"
      },
      "source": [
        "Results are worse. For instance, \"Shimon Peres\" is extended to \"minister Shimon Peres\" since minister has a compound relationship with Shimon, and that's clearly not the correct identification of the PERSON named entity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "NUI2ZAOLGSWb",
        "outputId": "8b564e9a-58e5-4d7f-8bfe-f7de7acdf3d7"
      },
      "source": [
        "# Evaluation of the results\n",
        "NE_ref, NE_pred = token_entities(docs)\n",
        "print(classification_report(NE_ref, NE_pred))\n",
        "print('='*80)\n",
        "\n",
        "NE_ref, NE_pred = chunk_entities(docs)\n",
        "results = conll.evaluate(NE_ref, NE_pred)\n",
        "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
        "pd_tbl.round(decimals=3)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.78      0.68      0.73      4025\n",
            "      B-MISC       0.12      0.57      0.20      1993\n",
            "       B-ORG       0.47      0.30      0.36      3588\n",
            "       B-PER       0.67      0.55      0.61      3920\n",
            "       I-LOC       0.49      0.60      0.54       651\n",
            "      I-MISC       0.06      0.31      0.09       722\n",
            "       I-ORG       0.43      0.55      0.48      1971\n",
            "       I-PER       0.66      0.83      0.73      2670\n",
            "           O       0.95      0.86      0.90     97195\n",
            "\n",
            "    accuracy                           0.81    116735\n",
            "   macro avg       0.51      0.58      0.52    116735\n",
            "weighted avg       0.88      0.81      0.84    116735\n",
            "\n",
            "================================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>p</th>\n",
              "      <th>r</th>\n",
              "      <th>f</th>\n",
              "      <th>s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>LOC</th>\n",
              "      <td>0.767</td>\n",
              "      <td>0.670</td>\n",
              "      <td>0.715</td>\n",
              "      <td>4025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MISC</th>\n",
              "      <td>0.116</td>\n",
              "      <td>0.539</td>\n",
              "      <td>0.190</td>\n",
              "      <td>1993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ORG</th>\n",
              "      <td>0.416</td>\n",
              "      <td>0.267</td>\n",
              "      <td>0.325</td>\n",
              "      <td>3588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PER</th>\n",
              "      <td>0.648</td>\n",
              "      <td>0.534</td>\n",
              "      <td>0.586</td>\n",
              "      <td>3920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>total</th>\n",
              "      <td>0.372</td>\n",
              "      <td>0.504</td>\n",
              "      <td>0.428</td>\n",
              "      <td>13526</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           p      r      f      s\n",
              "LOC    0.767  0.670  0.715   4025\n",
              "MISC   0.116  0.539  0.190   1993\n",
              "ORG    0.416  0.267  0.325   3588\n",
              "PER    0.648  0.534  0.586   3920\n",
              "total  0.372  0.504  0.428  13526"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}