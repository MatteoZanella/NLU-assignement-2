{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLU-assignment-2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoZanella/NLU-assignement-2/blob/main/NLU_assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O99OsUUONTEK"
      },
      "source": [
        "# NLU assignment n.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRkD3V65KQQZ"
      },
      "source": [
        "Update SpaCy to version 3 and download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvecUgE53Qqj"
      },
      "source": [
        "%%capture\n",
        "!pip install --upgrade spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!wget -nc https://raw.githubusercontent.com/esrel/NLU.Lab.2021/master/src/conll.py\n",
        "!wget -nc https://github.com/esrel/NLU.Lab.2021/raw/master/src/conll2003.zip\n",
        "!unzip -n conll2003.zip -d conll2003"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEGnfiiQKacq"
      },
      "source": [
        "Load the dataset and Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJVn2Qwsv6Vu"
      },
      "source": [
        "# Imports\n",
        "import random\n",
        "import conll\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from spacy.tokens import Token\n",
        "from spacy.training import Alignment\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "gt = conll.read_corpus_conll(\"./conll2003/train.txt\", fs=\" \")\n",
        "gt.extend(conll.read_corpus_conll(\"./conll2003/test.txt\", fs=\" \"))\n",
        "gt.extend(conll.read_corpus_conll(\"./conll2003/dev.txt\", fs=\" \"))\n",
        "\n",
        "# Removing reference lines\n",
        "gt = [tag_sent for tag_sent in gt if tag_sent[0][0] != '-DOCSTART-']\n",
        "\n",
        "# Limit the dataset, for a faster analysis in the following code\n",
        "gt = random.sample(gt, 8000)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13PCnS7ODoqV"
      },
      "source": [
        "Creation of custom extentions to save the dataset information directly in the SpaCy tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjkNY2ZbNvMg"
      },
      "source": [
        "Token.set_extension(\"ent_ref\", default='')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1UvP9ygYDAY"
      },
      "source": [
        "Translation function: By scanning the entire Conll dataset, you can see that the only Entities present are:\n",
        "`'LOC', 'ORG', 'PER', 'MISC'`\n",
        "\n",
        "SpaCy Entities, more detailed, should be translated according to their meaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDFcsnpNVXMC"
      },
      "source": [
        "def to_ref_entity(token):\n",
        "  ent_iob = token.ent_iob_\n",
        "  ent_type = token.ent_type_\n",
        "  if ent_type == 'ORG' : # Organizations\n",
        "    ent_type = 'ORG'\n",
        "  elif ent_type == 'PERSON':  # Persons\n",
        "    ent_type = 'PER'\n",
        "  elif ent_type == 'GPE' or ent_type == 'FAC' or ent_type == 'LOC':  # Localities\n",
        "    ent_type = 'LOC'\n",
        "  else:\n",
        "    ent_type = 'MISC'\n",
        "  \n",
        "  if ent_iob == 'O':\n",
        "    return ent_iob \n",
        "  else:\n",
        "    return f\"{ent_iob}-{ent_type}\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oaIaTn9E493"
      },
      "source": [
        "From the ground truth dataset, extract the corpus with all plain text sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj8oolMdEpRo"
      },
      "source": [
        "corpus = [\" \".join([tup[0] for tup in gt_sentence]) for gt_sentence in gt]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kDDUX8DPF_5"
      },
      "source": [
        "## Task 1: SpaCy NER evaluation\n",
        "Evaluate spaCy NER on CoNLL 2003 data (provided)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfusohC_YeGl"
      },
      "source": [
        "The spacy tokenization is different from the one provided in the dataset.\n",
        "I checked `alignment.x2y.lengths` and verified that spacy tokens needs to be merged at most, never to be splitted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbfFVtukdkrO"
      },
      "source": [
        "docs = []\n",
        "for gt_sentence in gt:\n",
        "  # List of ground truth tokens (token, POS, chunk, entity)\n",
        "  gt_tokens = [tup[0] for tup in gt_sentence]\n",
        "  # Create Doc object and extract tokens\n",
        "  doc = nlp(\" \".join(gt_tokens))\n",
        "  doc_tokens = [t.text for t in doc]\n",
        "  \n",
        "  # Get the alignment: .y2x.lengths has the merge informations\n",
        "  # .x2y.lengths is all ones with the tokenization considered\n",
        "  alignment = Alignment.from_strings(doc_tokens, gt_tokens)\n",
        "  # Merge together tokens to reflect ground truth tokenization\n",
        "  with doc.retokenize() as retokenizer:\n",
        "    doc_idx = 0\n",
        "    for length in alignment.y2x.lengths:\n",
        "      if length > 1:\n",
        "        retokenizer.merge(doc[doc_idx:doc_idx+length])\n",
        "      doc_idx += length\n",
        "\n",
        "  # Add the information about chunk division and entity\n",
        "  for token, ref in zip(doc, gt_sentence):\n",
        "    token._.ent_ref = ref[3]\n",
        "  docs.append(doc)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEpF0UVWNJ-c"
      },
      "source": [
        "### Part 1.1: Token-level performance\n",
        "Report token-level performance (per class and total)\n",
        "  - accuracy of correctly recognizing all tokens that belong to named entities (i.e. tag-level accuracy)\n",
        "  - to get per-class and total token-level performances you use scikit-learn's classification report, like we did in the lab on evaluation (you don't need to compute accuracy per-class, such thing does not exist)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3ga2XpUi6bo"
      },
      "source": [
        "def token_entities(docs):\n",
        "  \"\"\"Extract token-level predicted and reference Named Entities, as requested by classification_report\"\"\"\n",
        "  token_NE_ref = []\n",
        "  token_NE_pred = []\n",
        "\n",
        "  for doc in docs:\n",
        "    for token in doc:\n",
        "      token_NE_ref.append(token._.ent_ref)\n",
        "      token_NE_pred.append(to_ref_entity(token))\n",
        "  return token_NE_ref, token_NE_pred"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "tMXHHB6efcmv",
        "outputId": "23ef0026-b856-40af-a690-eb7b89197800"
      },
      "source": [
        "# Print the results\n",
        "NE_ref, NE_pred = token_entities(docs)\n",
        "print(classification_report(NE_ref, NE_pred))\n",
        "print('='*80)\n",
        "# Optional Confusion Matrix\n",
        "y_actu = pd.Series(NE_ref, name='Actual')\n",
        "y_pred = pd.Series(NE_pred, name='Predicted')\n",
        "pd_tbl = pd.crosstab(y_pred, y_actu)\n",
        "pd_tbl.round(decimals=3)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.78      0.70      0.74      4047\n",
            "      B-MISC       0.12      0.58      0.20      1899\n",
            "       B-ORG       0.48      0.31      0.38      3561\n",
            "       B-PER       0.80      0.66      0.73      3895\n",
            "       I-LOC       0.56      0.58      0.57       643\n",
            "      I-MISC       0.04      0.28      0.08       637\n",
            "       I-ORG       0.48      0.55      0.51      2037\n",
            "       I-PER       0.83      0.81      0.82      2680\n",
            "           O       0.95      0.87      0.91     97193\n",
            "\n",
            "    accuracy                           0.82    116592\n",
            "   macro avg       0.56      0.59      0.55    116592\n",
            "weighted avg       0.90      0.82      0.85    116592\n",
            "\n",
            "================================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Actual</th>\n",
              "      <th>B-LOC</th>\n",
              "      <th>B-MISC</th>\n",
              "      <th>B-ORG</th>\n",
              "      <th>B-PER</th>\n",
              "      <th>I-LOC</th>\n",
              "      <th>I-MISC</th>\n",
              "      <th>I-ORG</th>\n",
              "      <th>I-PER</th>\n",
              "      <th>O</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Predicted</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>B-LOC</th>\n",
              "      <td>2839</td>\n",
              "      <td>59</td>\n",
              "      <td>442</td>\n",
              "      <td>117</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>27</td>\n",
              "      <td>9</td>\n",
              "      <td>119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B-MISC</th>\n",
              "      <td>86</td>\n",
              "      <td>1103</td>\n",
              "      <td>74</td>\n",
              "      <td>46</td>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>22</td>\n",
              "      <td>4</td>\n",
              "      <td>7900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B-ORG</th>\n",
              "      <td>194</td>\n",
              "      <td>176</td>\n",
              "      <td>1113</td>\n",
              "      <td>315</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>19</td>\n",
              "      <td>3</td>\n",
              "      <td>475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B-PER</th>\n",
              "      <td>95</td>\n",
              "      <td>36</td>\n",
              "      <td>328</td>\n",
              "      <td>2584</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>32</td>\n",
              "      <td>48</td>\n",
              "      <td>81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-LOC</th>\n",
              "      <td>80</td>\n",
              "      <td>16</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>376</td>\n",
              "      <td>20</td>\n",
              "      <td>98</td>\n",
              "      <td>22</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-MISC</th>\n",
              "      <td>2</td>\n",
              "      <td>58</td>\n",
              "      <td>17</td>\n",
              "      <td>23</td>\n",
              "      <td>11</td>\n",
              "      <td>177</td>\n",
              "      <td>13</td>\n",
              "      <td>24</td>\n",
              "      <td>3636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-ORG</th>\n",
              "      <td>46</td>\n",
              "      <td>61</td>\n",
              "      <td>252</td>\n",
              "      <td>42</td>\n",
              "      <td>87</td>\n",
              "      <td>145</td>\n",
              "      <td>1114</td>\n",
              "      <td>123</td>\n",
              "      <td>461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-PER</th>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>34</td>\n",
              "      <td>30</td>\n",
              "      <td>22</td>\n",
              "      <td>211</td>\n",
              "      <td>2167</td>\n",
              "      <td>128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>O</th>\n",
              "      <td>696</td>\n",
              "      <td>386</td>\n",
              "      <td>1317</td>\n",
              "      <td>731</td>\n",
              "      <td>128</td>\n",
              "      <td>218</td>\n",
              "      <td>501</td>\n",
              "      <td>280</td>\n",
              "      <td>84340</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Actual     B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
              "Predicted                                                                 \n",
              "B-LOC       2839      59    442    117      7       7     27      9    119\n",
              "B-MISC        86    1103     74     46      0      34     22      4   7900\n",
              "B-ORG        194     176   1113    315      1       6     19      3    475\n",
              "B-PER         95      36    328   2584      3       8     32     48     81\n",
              "I-LOC         80      16      8      3    376      20     98     22     53\n",
              "I-MISC         2      58     17     23     11     177     13     24   3636\n",
              "I-ORG         46      61    252     42     87     145   1114    123    461\n",
              "I-PER          9       4     10     34     30      22    211   2167    128\n",
              "O            696     386   1317    731    128     218    501    280  84340"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCAHh9aBdHJr"
      },
      "source": [
        "### Part 1.2: Chunk-level performance\n",
        "Report CoNLL chunk-level performance (per class and total):\n",
        "  - Precision, recall, f-measure of correctly recognizing all the named entities in a chunk per class and total.\n",
        "  - To get chunk-level NER performance, you simply need to use conll.py's evaluate, that computes segmentation and labeling performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnDcyVvOaSq6"
      },
      "source": [
        "def chunk_entities(docs):\n",
        "  \"\"\"Transform tokens into (text, iob), as requested by conll.evaluate()\"\"\"\n",
        "  chunk_NE_ref = []\n",
        "  chunk_NE_pred = []\n",
        "  \n",
        "  for doc in docs:\n",
        "    chunk_NE_pred.append([(t.text, to_ref_entity(t)) for t in doc])\n",
        "    chunk_NE_ref.append([(t.text, t._.ent_ref) for t in doc])\n",
        "  \n",
        "  return chunk_NE_ref, chunk_NE_pred"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "LQTFrPuLScyF",
        "outputId": "f468ce98-c919-43eb-9c97-e12852d32d53"
      },
      "source": [
        "# Print the results\n",
        "NE_ref, NE_pred = chunk_entities(docs)\n",
        "results = conll.evaluate(NE_ref, NE_pred)\n",
        "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
        "pd_tbl.round(decimals=3)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>p</th>\n",
              "      <th>r</th>\n",
              "      <th>f</th>\n",
              "      <th>s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>PER</th>\n",
              "      <td>0.784</td>\n",
              "      <td>0.647</td>\n",
              "      <td>0.709</td>\n",
              "      <td>3895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ORG</th>\n",
              "      <td>0.428</td>\n",
              "      <td>0.277</td>\n",
              "      <td>0.336</td>\n",
              "      <td>3561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LOC</th>\n",
              "      <td>0.774</td>\n",
              "      <td>0.693</td>\n",
              "      <td>0.731</td>\n",
              "      <td>4047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MISC</th>\n",
              "      <td>0.112</td>\n",
              "      <td>0.548</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>total</th>\n",
              "      <td>0.399</td>\n",
              "      <td>0.549</td>\n",
              "      <td>0.462</td>\n",
              "      <td>13402</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           p      r      f      s\n",
              "PER    0.784  0.647  0.709   3895\n",
              "ORG    0.428  0.277  0.336   3561\n",
              "LOC    0.774  0.693  0.731   4047\n",
              "MISC   0.112  0.548  0.186   1899\n",
              "total  0.399  0.549  0.462  13402"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH-y2WbUPZu5"
      },
      "source": [
        "## Task 2: Grouping of Entities\n",
        "Write a function to group recognized named entities using noun_chunks method of [spaCy](https://spacy.io/usage/linguistic-features#noun-chunks)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoXv9m1G5j9q"
      },
      "source": [
        "def grouped_entities(sentence):\n",
        "  doc = nlp(sentence)\n",
        "  n_chunks = list(doc.noun_chunks)\n",
        "  entities = []\n",
        "  \n",
        "  curr_chunk = 0\n",
        "  curr_token = 0\n",
        "  chunk_ents = set()\n",
        "  for token in doc:\n",
        "    if curr_chunk < len(n_chunks) and token == n_chunks[curr_chunk][curr_token]:  # Token is next in the noun chunk\n",
        "      if token.ent_type_ != '': # Middle or final token\n",
        "        chunk_ents.add(token.ent_type_)\n",
        "      curr_token += 1\n",
        "      if token == n_chunks[curr_chunk][-1]:  # Last token of the current chunk\n",
        "        if len(chunk_ents) > 0:\n",
        "          entities.append(sorted(chunk_ents))  # Sorted, so it's a list without notion of token ordering\n",
        "        curr_chunk += 1  # Look for next token\n",
        "        curr_token = 0  # At the first position\n",
        "        chunk_ents = set()  # Reset the set of chunk's tokens\n",
        "    elif token.ent_type_ != '':\n",
        "      entities.append([token.ent_type_])\n",
        "  return entities"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6JyWC2CAc76",
        "outputId": "5585650a-487d-4285-dc7d-02bcdd646be2"
      },
      "source": [
        "# Testing the function\n",
        "grouped_entities(\"Apple's Steve Jobs died in 2011 in Palo Alto, California.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['ORG', 'PERSON'], ['DATE'], ['GPE'], ['GPE']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVe_vI8Y5Q-8"
      },
      "source": [
        "### Part 2.1: Frequency analysis\n",
        "Analyze the groups in terms of most frequent combinations (i.e. NER types that go together)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wwFAQUSFUAk"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "frequencies = Counter()\n",
        "for sentence in corpus: \n",
        "  entities = grouped_entities(sentence)\n",
        "  for group in entities:\n",
        "    combination = '-'.join(group)\n",
        "    frequencies[combination] += 1"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQwW55uefzHt",
        "outputId": "0dc2158f-30f7-4abf-e57a-646f8841485b"
      },
      "source": [
        "for combination, counter in frequencies.most_common():\n",
        "  print(f\"{combination}: {counter}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DATE: 4714\n",
            "CARDINAL: 3883\n",
            "GPE: 3168\n",
            "PERSON: 2954\n",
            "ORG: 2419\n",
            "NORP: 859\n",
            "MONEY: 627\n",
            "ORDINAL: 404\n",
            "TIME: 397\n",
            "QUANTITY: 246\n",
            "PERCENT: 226\n",
            "LOC: 130\n",
            "EVENT: 113\n",
            "NORP-PERSON: 107\n",
            "CARDINAL-PERSON: 106\n",
            "GPE-PERSON: 92\n",
            "ORG-PERSON: 71\n",
            "WORK_OF_ART: 70\n",
            "PRODUCT: 67\n",
            "FAC: 65\n",
            "CARDINAL-ORG: 55\n",
            "LAW: 51\n",
            "CARDINAL-NORP: 43\n",
            "GPE-ORG: 41\n",
            "CARDINAL-GPE: 33\n",
            "DATE-ORG: 31\n",
            "GPE-ORDINAL: 25\n",
            "NORP-ORDINAL: 21\n",
            "DATE-GPE: 20\n",
            "DATE-PERSON: 19\n",
            "LANGUAGE: 18\n",
            "NORP-ORG: 17\n",
            "CARDINAL-DATE: 17\n",
            "GPE-NORP: 17\n",
            "DATE-EVENT: 17\n",
            "DATE-TIME: 16\n",
            "ORDINAL-PERSON: 13\n",
            "DATE-NORP: 12\n",
            "GPE-PRODUCT: 11\n",
            "ORDINAL-ORG: 10\n",
            "CARDINAL-ORDINAL: 8\n",
            "ORG-PRODUCT: 8\n",
            "LANGUAGE-ORDINAL: 7\n",
            "ORDINAL-QUANTITY: 6\n",
            "DATE-ORDINAL: 6\n",
            "CARDINAL-PRODUCT: 5\n",
            "EVENT-GPE: 4\n",
            "GPE-LOC: 4\n",
            "CARDINAL-GPE-PERSON: 3\n",
            "DATE-PERCENT: 3\n",
            "FAC-GPE: 3\n",
            "GPE-ORDINAL-PERSON: 3\n",
            "MONEY-ORG-PRODUCT: 3\n",
            "GPE-NORP-ORG: 3\n",
            "GPE-ORG-PERSON: 2\n",
            "LOC-NORP: 2\n",
            "CARDINAL-QUANTITY: 2\n",
            "CARDINAL-PERCENT: 2\n",
            "DATE-PRODUCT: 2\n",
            "NORP-ORG-PERSON: 2\n",
            "DATE-WORK_OF_ART: 2\n",
            "CARDINAL-MONEY-ORG: 2\n",
            "PERSON-WORK_OF_ART: 2\n",
            "PERSON-TIME: 2\n",
            "DATE-GPE-PERSON: 2\n",
            "MONEY-PERSON: 2\n",
            "NORP-PRODUCT: 2\n",
            "DATE-MONEY: 2\n",
            "EVENT-ORDINAL: 2\n",
            "NORP-ORDINAL-PERSON: 2\n",
            "FAC-ORG: 1\n",
            "NORP-ORDINAL-ORG: 1\n",
            "DATE-GPE-ORG: 1\n",
            "CARDINAL-GPE-MONEY-ORG: 1\n",
            "GPE-NORP-PERSON: 1\n",
            "DATE-EVENT-GPE: 1\n",
            "CARDINAL-ORG-PERSON: 1\n",
            "ORDINAL-TIME: 1\n",
            "CARDINAL-LOC: 1\n",
            "CARDINAL-GPE-ORG: 1\n",
            "CARDINAL-EVENT: 1\n",
            "LANGUAGE-NORP: 1\n",
            "MONEY-NORP: 1\n",
            "DATE-GPE-NORP: 1\n",
            "DATE-NORP-ORG: 1\n",
            "DATE-LOC: 1\n",
            "EVENT-ORG: 1\n",
            "GPE-NORP-ORDINAL: 1\n",
            "CARDINAL-FAC: 1\n",
            "LANGUAGE-ORG: 1\n",
            "DATE-LANGUAGE: 1\n",
            "NORP-PERCENT: 1\n",
            "CARDINAL-GPE-TIME: 1\n",
            "CARDINAL-ORG-TIME: 1\n",
            "DATE-FAC: 1\n",
            "CARDINAL-ORG-PRODUCT: 1\n",
            "EVENT-NORP: 1\n",
            "LANGUAGE-ORDINAL-ORG: 1\n",
            "CARDINAL-LANGUAGE-PERSON: 1\n",
            "LOC-ORDINAL: 1\n",
            "MONEY-ORDINAL: 1\n",
            "NORP-QUANTITY: 1\n",
            "CARDINAL-GPE-NORP: 1\n",
            "ORDINAL-ORG-PERSON: 1\n",
            "ORDINAL-PRODUCT: 1\n",
            "CARDINAL-MONEY: 1\n",
            "DATE-NORP-PERSON: 1\n",
            "GPE-ORDINAL-ORG: 1\n",
            "LAW-ORG-PERSON: 1\n",
            "MONEY-PRODUCT: 1\n",
            "ORG-QUANTITY: 1\n",
            "MONEY-ORG: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfn7btkXPfSs"
      },
      "source": [
        "## Task 3\n",
        "One of the possible post-processing steps is to fix segmentation errors. Write a function that extends the entity span to cover the full noun-compounds. Make use of compound dependency relation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ9uvCQDneGk"
      },
      "source": [
        "You have to be careful when extending entities with the coumpound, because you could overwrite other entities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-1HNIHrju0X"
      },
      "source": [
        "from spacy.tokens import Span\n",
        "\n",
        "def expand_entities(doc):\n",
        "  entities = []\n",
        "  for ents_i, ent in enumerate(doc.ents):\n",
        "    ent_start = ent.start\n",
        "    ent_end = ent.end\n",
        "    # List of all the children of the entity span tokens\n",
        "    subtree = list(ent.root.subtree) \n",
        "    search_start = subtree[0].i\n",
        "    search_end = subtree[-1].i + 1\n",
        "    # The search should be limited by previous and next entities\n",
        "    if ents_i > 0 and doc.ents[ents_i - 1].end > search_start:\n",
        "      search_start = doc.ents[ents_i - 1].end\n",
        "    if ents_i < (len(doc.ents) - 1) and doc.ents[ents_i + 1].start < search_end:\n",
        "      search_end = doc.ents[ents_i + 1].start\n",
        "    # Extend the head\n",
        "    token = doc[search_start]\n",
        "    while token.i < ent_start:\n",
        "      compound_root = token\n",
        "      while compound_root.dep_ == 'compound':\n",
        "        compound_root = compound_root.head\n",
        "      if ent_start <= compound_root.i < ent_end:\n",
        "        ent_start = token.i\n",
        "      token = token.nbor()\n",
        "    # Extend the tail\n",
        "    token = doc[search_end - 1]\n",
        "    while token.i >= ent_end:\n",
        "      compound_root = token\n",
        "      while compound_root.dep_ == 'compound':\n",
        "        compound_root = compound_root.head\n",
        "      if ent_start <= compound_root.i < ent_end:\n",
        "        ent_end = token.i + 1\n",
        "      token = token.nbor(-1)\n",
        "    # Add the expanded entity to the list\n",
        "    entity = Span(doc, ent_start, ent_end, label=ent.label_)\n",
        "    entities.append(entity)\n",
        "  # Set the extended entities\n",
        "  doc.set_ents(entities)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tpKTz-CB-t7"
      },
      "source": [
        "We can directy apply the postprocessing to the docs object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EudFK_mzJ_1s"
      },
      "source": [
        "# Application of the post-processing step\n",
        "for doc in docs:\n",
        "  expand_entities(doc)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSXVOvA4CImE"
      },
      "source": [
        "Results are worse. For instance, \"Shimon Peres\" is extended to \"minister Shimon Peres\" since minister has a compound relationship with Shimon, and that's clearly not the correct identification of the PERSON named entity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "NUI2ZAOLGSWb",
        "outputId": "11dc3b9b-759f-44e7-edb4-1f01788b55a5"
      },
      "source": [
        "# Evaluation of the results\n",
        "NE_ref, NE_pred = token_entities(docs)\n",
        "print(classification_report(NE_ref, NE_pred))\n",
        "print('='*80)\n",
        "\n",
        "NE_ref, NE_pred = chunk_entities(docs)\n",
        "results = conll.evaluate(NE_ref, NE_pred)\n",
        "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
        "pd_tbl.round(decimals=3)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.77      0.69      0.73      4047\n",
            "      B-MISC       0.12      0.58      0.20      1899\n",
            "       B-ORG       0.47      0.31      0.37      3561\n",
            "       B-PER       0.69      0.57      0.62      3895\n",
            "       I-LOC       0.49      0.58      0.54       643\n",
            "      I-MISC       0.04      0.28      0.08       637\n",
            "       I-ORG       0.46      0.55      0.50      2037\n",
            "       I-PER       0.67      0.82      0.74      2680\n",
            "           O       0.95      0.86      0.90     97193\n",
            "\n",
            "    accuracy                           0.81    116592\n",
            "   macro avg       0.52      0.58      0.52    116592\n",
            "weighted avg       0.89      0.81      0.84    116592\n",
            "\n",
            "================================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>p</th>\n",
              "      <th>r</th>\n",
              "      <th>f</th>\n",
              "      <th>s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>PER</th>\n",
              "      <td>0.666</td>\n",
              "      <td>0.550</td>\n",
              "      <td>0.603</td>\n",
              "      <td>3895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ORG</th>\n",
              "      <td>0.417</td>\n",
              "      <td>0.270</td>\n",
              "      <td>0.328</td>\n",
              "      <td>3561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LOC</th>\n",
              "      <td>0.763</td>\n",
              "      <td>0.683</td>\n",
              "      <td>0.721</td>\n",
              "      <td>4047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MISC</th>\n",
              "      <td>0.112</td>\n",
              "      <td>0.545</td>\n",
              "      <td>0.185</td>\n",
              "      <td>1899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>total</th>\n",
              "      <td>0.375</td>\n",
              "      <td>0.515</td>\n",
              "      <td>0.434</td>\n",
              "      <td>13402</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           p      r      f      s\n",
              "PER    0.666  0.550  0.603   3895\n",
              "ORG    0.417  0.270  0.328   3561\n",
              "LOC    0.763  0.683  0.721   4047\n",
              "MISC   0.112  0.545  0.185   1899\n",
              "total  0.375  0.515  0.434  13402"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}