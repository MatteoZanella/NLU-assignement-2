{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLU-assignment-2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoZanella/NLU-assignement-2/blob/main/NLU_assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O99OsUUONTEK"
      },
      "source": [
        "# NLU assignment n.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRkD3V65KQQZ"
      },
      "source": [
        "Update SpaCy to version 3 and download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvecUgE53Qqj"
      },
      "source": [
        "%%capture\n",
        "!pip install --upgrade spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!wget -nc https://raw.githubusercontent.com/esrel/NLU.Lab.2021/master/src/conll.py\n",
        "!wget -nc https://github.com/esrel/NLU.Lab.2021/raw/master/src/conll2003.zip\n",
        "!unzip -n conll2003.zip -d conll2003"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEGnfiiQKacq"
      },
      "source": [
        "Load the dataset and Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJVn2Qwsv6Vu"
      },
      "source": [
        "# Imports\n",
        "import random\n",
        "import conll\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from spacy.tokens import Token\n",
        "from spacy.training import Alignment\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "gt = conll.read_corpus_conll(\"./conll2003/train.txt\", fs=\" \")\n",
        "gt.extend(conll.read_corpus_conll(\"./conll2003/test.txt\", fs=\" \"))\n",
        "gt.extend(conll.read_corpus_conll(\"./conll2003/dev.txt\", fs=\" \"))\n",
        "\n",
        "# Removing reference lines\n",
        "gt = [tag_sent for tag_sent in gt if tag_sent[0][0] != '-DOCSTART-']\n",
        "\n",
        "# Limit the dataset, for a faster analysis in the following code\n",
        "gt = random.sample(gt, 8000)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kDDUX8DPF_5"
      },
      "source": [
        "## Task 1: SpaCy NER evaluation\n",
        "Evaluate spaCy NER on CoNLL 2003 data (provided)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oA4UkLBrJvm"
      },
      "source": [
        "### Part 1.1: SpaCy NER and alignement\n",
        "As first step, we need to evaluate the sentences with SpaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13PCnS7ODoqV"
      },
      "source": [
        "Creation of custom extentions to save the dataset information directly in the SpaCy tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjkNY2ZbNvMg"
      },
      "source": [
        "Token.set_extension(\"ent_ref\", default='')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1UvP9ygYDAY"
      },
      "source": [
        "Translation function: By scanning the entire Conll dataset, you can see that the only Entities present are:\n",
        "`'LOC', 'ORG', 'PER', 'MISC'`\n",
        "\n",
        "SpaCy Entities, more detailed, should be translated according to their meaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDFcsnpNVXMC"
      },
      "source": [
        "def to_ref_entity(token):\n",
        "  ent_iob = token.ent_iob_\n",
        "  ent_type = token.ent_type_\n",
        "  if ent_type == 'ORG': # Organizations\n",
        "    ent_type = 'ORG'\n",
        "  elif ent_type == 'PERSON':  # Persons\n",
        "    ent_type = 'PER'\n",
        "  elif ent_type == 'GPE' or ent_type == 'FAC' or ent_type == 'LOC':  # Localities\n",
        "    ent_type = 'LOC'\n",
        "  else:\n",
        "    ent_type = 'MISC'\n",
        "  \n",
        "  if ent_iob == 'O':\n",
        "    return ent_iob \n",
        "  else:\n",
        "    return f\"{ent_iob}-{ent_type}\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfusohC_YeGl"
      },
      "source": [
        "The spacy tokenization is different from the one provided in the dataset.\n",
        "I checked `alignment.x2y.lengths` and verified that spacy tokens needs to be merged at most, never to be splitted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbfFVtukdkrO"
      },
      "source": [
        "docs = []\n",
        "for gt_sentence in gt:\n",
        "  # List of ground truth tokens (token, POS, chunk, entity)\n",
        "  gt_tokens = [tup[0] for tup in gt_sentence]\n",
        "  # Create Doc object and extract tokens\n",
        "  doc = nlp(\" \".join(gt_tokens))\n",
        "  doc_tokens = [t.text for t in doc]\n",
        "  \n",
        "  # Get the alignment: .y2x.lengths has the merge informations\n",
        "  # .x2y.lengths is all ones with the tokenization considered\n",
        "  alignment = Alignment.from_strings(doc_tokens, gt_tokens)\n",
        "  # Merge together tokens to reflect ground truth tokenization\n",
        "  with doc.retokenize() as retokenizer:\n",
        "    doc_idx = 0\n",
        "    for length in alignment.y2x.lengths:\n",
        "      if length > 1:\n",
        "        retokenizer.merge(doc[doc_idx:doc_idx+length])\n",
        "      doc_idx += length\n",
        "\n",
        "  # Add the information about chunk division and entity\n",
        "  for token, ref in zip(doc, gt_sentence):\n",
        "    token._.ent_ref = ref[3]\n",
        "  docs.append(doc)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEpF0UVWNJ-c"
      },
      "source": [
        "### Part 1.2: Token-level performance\n",
        "Report token-level performance (per class and total)\n",
        "  - accuracy of correctly recognizing all tokens that belong to named entities (i.e. tag-level accuracy)\n",
        "  - to get per-class and total token-level performances you use scikit-learn's classification report, like we did in the lab on evaluation (you don't need to compute accuracy per-class, such thing does not exist)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3ga2XpUi6bo"
      },
      "source": [
        "def token_entities(docs):\n",
        "  \"\"\"Extract token-level predicted and reference Named Entities, as requested by classification_report\"\"\"\n",
        "  token_NE_ref = []\n",
        "  token_NE_pred = []\n",
        "\n",
        "  for doc in docs:\n",
        "    for token in doc:\n",
        "      token_NE_ref.append(token._.ent_ref)\n",
        "      token_NE_pred.append(to_ref_entity(token))\n",
        "  return token_NE_ref, token_NE_pred"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "tMXHHB6efcmv",
        "outputId": "60539999-34f4-4171-eb72-212f7a1d7840"
      },
      "source": [
        "# Print the results\n",
        "NE_ref, NE_pred = token_entities(docs)\n",
        "print(classification_report(NE_ref, NE_pred))\n",
        "print('='*80)\n",
        "# Optional Confusion Matrix\n",
        "y_actu = pd.Series(NE_ref, name='Actual')\n",
        "y_pred = pd.Series(NE_pred, name='Predicted')\n",
        "pd_tbl = pd.crosstab(y_pred, y_actu)\n",
        "pd_tbl.round(decimals=3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.80      0.70      0.75      4074\n",
            "      B-MISC       0.13      0.61      0.21      2000\n",
            "       B-ORG       0.49      0.32      0.39      3636\n",
            "       B-PER       0.81      0.67      0.73      3900\n",
            "       I-LOC       0.53      0.59      0.56       593\n",
            "      I-MISC       0.05      0.29      0.09       696\n",
            "       I-ORG       0.48      0.56      0.52      2061\n",
            "       I-PER       0.84      0.81      0.82      2785\n",
            "           O       0.95      0.87      0.91     96735\n",
            "\n",
            "    accuracy                           0.82    116480\n",
            "   macro avg       0.56      0.60      0.55    116480\n",
            "weighted avg       0.89      0.82      0.85    116480\n",
            "\n",
            "================================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Actual</th>\n",
              "      <th>B-LOC</th>\n",
              "      <th>B-MISC</th>\n",
              "      <th>B-ORG</th>\n",
              "      <th>B-PER</th>\n",
              "      <th>I-LOC</th>\n",
              "      <th>I-MISC</th>\n",
              "      <th>I-ORG</th>\n",
              "      <th>I-PER</th>\n",
              "      <th>O</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Predicted</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>B-LOC</th>\n",
              "      <td>2861</td>\n",
              "      <td>57</td>\n",
              "      <td>401</td>\n",
              "      <td>116</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>18</td>\n",
              "      <td>9</td>\n",
              "      <td>107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B-MISC</th>\n",
              "      <td>81</td>\n",
              "      <td>1212</td>\n",
              "      <td>63</td>\n",
              "      <td>47</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>20</td>\n",
              "      <td>2</td>\n",
              "      <td>7989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B-ORG</th>\n",
              "      <td>190</td>\n",
              "      <td>177</td>\n",
              "      <td>1168</td>\n",
              "      <td>309</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>28</td>\n",
              "      <td>4</td>\n",
              "      <td>489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B-PER</th>\n",
              "      <td>92</td>\n",
              "      <td>49</td>\n",
              "      <td>316</td>\n",
              "      <td>2599</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>54</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-LOC</th>\n",
              "      <td>72</td>\n",
              "      <td>13</td>\n",
              "      <td>11</td>\n",
              "      <td>6</td>\n",
              "      <td>352</td>\n",
              "      <td>22</td>\n",
              "      <td>93</td>\n",
              "      <td>28</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-MISC</th>\n",
              "      <td>1</td>\n",
              "      <td>61</td>\n",
              "      <td>17</td>\n",
              "      <td>27</td>\n",
              "      <td>3</td>\n",
              "      <td>204</td>\n",
              "      <td>23</td>\n",
              "      <td>27</td>\n",
              "      <td>3532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-ORG</th>\n",
              "      <td>52</td>\n",
              "      <td>60</td>\n",
              "      <td>285</td>\n",
              "      <td>34</td>\n",
              "      <td>80</td>\n",
              "      <td>141</td>\n",
              "      <td>1159</td>\n",
              "      <td>130</td>\n",
              "      <td>479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-PER</th>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>38</td>\n",
              "      <td>31</td>\n",
              "      <td>30</td>\n",
              "      <td>182</td>\n",
              "      <td>2246</td>\n",
              "      <td>121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>O</th>\n",
              "      <td>716</td>\n",
              "      <td>369</td>\n",
              "      <td>1367</td>\n",
              "      <td>724</td>\n",
              "      <td>117</td>\n",
              "      <td>245</td>\n",
              "      <td>519</td>\n",
              "      <td>285</td>\n",
              "      <td>83876</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Actual     B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
              "Predicted                                                                 \n",
              "B-LOC       2861      57    401    116      8       7     18      9    107\n",
              "B-MISC        81    1212     63     47      0      35     20      2   7989\n",
              "B-ORG        190     177   1168    309      1       8     28      4    489\n",
              "B-PER         92      49    316   2599      1       4     19     54     75\n",
              "I-LOC         72      13     11      6    352      22     93     28     67\n",
              "I-MISC         1      61     17     27      3     204     23     27   3532\n",
              "I-ORG         52      60    285     34     80     141   1159    130    479\n",
              "I-PER          9       2      8     38     31      30    182   2246    121\n",
              "O            716     369   1367    724    117     245    519    285  83876"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCAHh9aBdHJr"
      },
      "source": [
        "### Part 1.3: Chunk-level performance\n",
        "Report CoNLL chunk-level performance (per class and total):\n",
        "  - Precision, recall, f-measure of correctly recognizing all the named entities in a chunk per class and total.\n",
        "  - To get chunk-level NER performance, you simply need to use conll.py's evaluate, that computes segmentation and labeling performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnDcyVvOaSq6"
      },
      "source": [
        "def chunk_entities(docs):\n",
        "  \"\"\"Transform tokens into (text, iob), as requested by conll.evaluate()\"\"\"\n",
        "  chunk_NE_ref = []\n",
        "  chunk_NE_pred = []\n",
        "  \n",
        "  for doc in docs:\n",
        "    chunk_NE_pred.append([(t.text, to_ref_entity(t)) for t in doc])\n",
        "    chunk_NE_ref.append([(t.text, t._.ent_ref) for t in doc])\n",
        "  \n",
        "  return chunk_NE_ref, chunk_NE_pred"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "LQTFrPuLScyF",
        "outputId": "b595042e-75fb-4b36-aff5-8cc63640e42b"
      },
      "source": [
        "# Print the results\n",
        "NE_ref, NE_pred = chunk_entities(docs)\n",
        "results = conll.evaluate(NE_ref, NE_pred)\n",
        "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
        "pd_tbl.round(decimals=3)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>p</th>\n",
              "      <th>r</th>\n",
              "      <th>f</th>\n",
              "      <th>s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ORG</th>\n",
              "      <td>0.444</td>\n",
              "      <td>0.290</td>\n",
              "      <td>0.351</td>\n",
              "      <td>3636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MISC</th>\n",
              "      <td>0.122</td>\n",
              "      <td>0.578</td>\n",
              "      <td>0.202</td>\n",
              "      <td>2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PER</th>\n",
              "      <td>0.791</td>\n",
              "      <td>0.651</td>\n",
              "      <td>0.714</td>\n",
              "      <td>3900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LOC</th>\n",
              "      <td>0.786</td>\n",
              "      <td>0.691</td>\n",
              "      <td>0.736</td>\n",
              "      <td>4074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>total</th>\n",
              "      <td>0.406</td>\n",
              "      <td>0.556</td>\n",
              "      <td>0.470</td>\n",
              "      <td>13610</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           p      r      f      s\n",
              "ORG    0.444  0.290  0.351   3636\n",
              "MISC   0.122  0.578  0.202   2000\n",
              "PER    0.791  0.651  0.714   3900\n",
              "LOC    0.786  0.691  0.736   4074\n",
              "total  0.406  0.556  0.470  13610"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH-y2WbUPZu5"
      },
      "source": [
        "## Task 2: Grouping of Entities\n",
        "Write a function to group recognized named entities using noun_chunks method of [spaCy](https://spacy.io/usage/linguistic-features#noun-chunks)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoXv9m1G5j9q"
      },
      "source": [
        "def grouped_entities(sentence):\n",
        "  doc = nlp(sentence)\n",
        "  n_chunks = list(doc.noun_chunks)\n",
        "  entities = []\n",
        "  \n",
        "  curr_chunk = 0  # Next chunk to be explored\n",
        "  curr_token = 0  # Within the chunk, which token exactly\n",
        "  chunk_ents = set()\n",
        "  for token in doc:\n",
        "    if curr_chunk < len(n_chunks) and token == n_chunks[curr_chunk][curr_token]:  # Token is next in the noun chunk\n",
        "      if token.ent_type_ != '': # Middle or final token\n",
        "        chunk_ents.add(token.ent_type_)\n",
        "      curr_token += 1\n",
        "      if token == n_chunks[curr_chunk][-1]:  # Last token of the current chunk\n",
        "        if len(chunk_ents) > 0:\n",
        "          entities.append(sorted(chunk_ents))  # Sorted, so it's a list without notion of token ordering\n",
        "        curr_chunk += 1  # Look for next token\n",
        "        curr_token = 0  # At the first position\n",
        "        chunk_ents = set()  # Reset the set of chunk's tokens\n",
        "    elif token.ent_type_ != '':\n",
        "      entities.append([token.ent_type_])\n",
        "  return entities"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6JyWC2CAc76",
        "outputId": "102233a4-22db-49ca-993c-812431cfb432"
      },
      "source": [
        "# Testing the function\n",
        "grouped_entities(\"Apple's Steve Jobs died in 2011 in Palo Alto, California.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['ORG', 'PERSON'], ['DATE'], ['GPE'], ['GPE']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVe_vI8Y5Q-8"
      },
      "source": [
        "### Part 2.1: Frequency analysis\n",
        "Analyze the groups in terms of most frequent combinations (i.e. NER types that go together)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oaIaTn9E493"
      },
      "source": [
        "From the ground truth dataset, extract the corpus with all plain text sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj8oolMdEpRo"
      },
      "source": [
        "corpus = [\" \".join([tup[0] for tup in gt_sentence]) for gt_sentence in gt]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wwFAQUSFUAk"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "frequencies = Counter()\n",
        "for sentence in corpus: \n",
        "  entities = grouped_entities(sentence)\n",
        "  for group in entities:\n",
        "    combination = '-'.join(group)\n",
        "    frequencies[combination] += 1"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQwW55uefzHt",
        "outputId": "13c9e8c1-7d69-48e3-8959-e54895a71dd4"
      },
      "source": [
        "for combination, counter in frequencies.most_common():\n",
        "  print(f\"{combination}: {counter}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DATE: 4694\n",
            "CARDINAL: 3897\n",
            "GPE: 3121\n",
            "PERSON: 2924\n",
            "ORG: 2423\n",
            "NORP: 888\n",
            "MONEY: 544\n",
            "TIME: 432\n",
            "ORDINAL: 417\n",
            "PERCENT: 276\n",
            "QUANTITY: 240\n",
            "LOC: 129\n",
            "NORP-PERSON: 129\n",
            "CARDINAL-PERSON: 95\n",
            "EVENT: 93\n",
            "GPE-PERSON: 85\n",
            "ORG-PERSON: 67\n",
            "CARDINAL-ORG: 64\n",
            "FAC: 64\n",
            "PRODUCT: 62\n",
            "WORK_OF_ART: 52\n",
            "CARDINAL-NORP: 48\n",
            "GPE-ORG: 44\n",
            "LAW: 42\n",
            "CARDINAL-GPE: 33\n",
            "DATE-ORG: 33\n",
            "NORP-ORG: 29\n",
            "DATE-GPE: 24\n",
            "LANGUAGE: 23\n",
            "DATE-TIME: 20\n",
            "ORDINAL-PERSON: 20\n",
            "GPE-NORP: 19\n",
            "NORP-ORDINAL: 18\n",
            "DATE-PERSON: 18\n",
            "GPE-ORDINAL: 16\n",
            "CARDINAL-DATE: 16\n",
            "ORDINAL-ORG: 14\n",
            "GPE-PRODUCT: 14\n",
            "DATE-NORP: 13\n",
            "DATE-EVENT: 12\n",
            "CARDINAL-ORDINAL: 11\n",
            "GPE-LOC: 10\n",
            "ORG-PRODUCT: 10\n",
            "LANGUAGE-ORDINAL: 9\n",
            "CARDINAL-PRODUCT: 6\n",
            "EVENT-NORP: 6\n",
            "FAC-GPE: 6\n",
            "DATE-NORP-PERSON: 6\n",
            "DATE-ORDINAL: 5\n",
            "DATE-PERCENT: 5\n",
            "ORDINAL-QUANTITY: 5\n",
            "CARDINAL-EVENT: 4\n",
            "DATE-MONEY: 3\n",
            "MONEY-PERSON: 3\n",
            "LOC-PERSON: 3\n",
            "MONEY-ORG: 3\n",
            "PERSON-WORK_OF_ART: 3\n",
            "NORP-PRODUCT: 3\n",
            "CARDINAL-PERCENT: 3\n",
            "NORP-ORG-PERSON: 2\n",
            "MONEY-ORG-PRODUCT: 2\n",
            "DATE-LOC: 2\n",
            "CARDINAL-FAC: 2\n",
            "EVENT-ORDINAL: 2\n",
            "DATE-QUANTITY: 2\n",
            "GPE-ORDINAL-PERSON: 2\n",
            "EVENT-PERSON: 2\n",
            "CARDINAL-NORP-PERSON: 1\n",
            "CARDINAL-ORDINAL-PERSON: 1\n",
            "DATE-EVENT-GPE: 1\n",
            "DATE-PRODUCT: 1\n",
            "FAC-PERSON: 1\n",
            "CARDINAL-LOC: 1\n",
            "DATE-WORK_OF_ART: 1\n",
            "GPE-PERCENT: 1\n",
            "CARDINAL-GPE-LOC-NORP-ORG: 1\n",
            "DATE-LANGUAGE-ORDINAL: 1\n",
            "CARDINAL-GPE-PERSON: 1\n",
            "GPE-LOC-ORG: 1\n",
            "DATE-PERSON-QUANTITY: 1\n",
            "CARDINAL-DATE-GPE: 1\n",
            "EVENT-GPE: 1\n",
            "NORP-ORDINAL-PERSON: 1\n",
            "CARDINAL-GPE-NORP: 1\n",
            "PERSON-PRODUCT: 1\n",
            "CARDINAL-DATE-ORDINAL: 1\n",
            "GPE-NORP-PERSON: 1\n",
            "DATE-GPE-PERSON: 1\n",
            "CARDINAL-MONEY: 1\n",
            "GPE-MONEY: 1\n",
            "ORG-TIME: 1\n",
            "LOC-NORP: 1\n",
            "GPE-ORG-PERSON: 1\n",
            "PERSON-TIME: 1\n",
            "CARDINAL-DATE-GPE-ORG: 1\n",
            "CARDINAL-ORDINAL-ORG: 1\n",
            "FAC-ORG: 1\n",
            "CARDINAL-GPE-ORG: 1\n",
            "DATE-NORP-ORG: 1\n",
            "CARDINAL-GPE-LOC-ORG: 1\n",
            "NORP-TIME: 1\n",
            "DATE-FAC: 1\n",
            "CARDINAL-DATE-PERSON: 1\n",
            "MONEY-PRODUCT: 1\n",
            "DATE-GPE-NORP-PERSON: 1\n",
            "LANGUAGE-NORP: 1\n",
            "ORDINAL-TIME: 1\n",
            "ORG-WORK_OF_ART: 1\n",
            "GPE-TIME: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfn7btkXPfSs"
      },
      "source": [
        "## Task 3: Covering full noun-compounds\n",
        "One of the possible post-processing steps is to fix segmentation errors. Write a function that extends the entity span to cover the full noun-compounds. Make use of compound dependency relation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ9uvCQDneGk"
      },
      "source": [
        "You have to be careful when extending entities with the coumpound, because you could overwrite other entities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-1HNIHrju0X"
      },
      "source": [
        "from spacy.tokens import Span\n",
        "\n",
        "def expand_entities(doc):\n",
        "  entities = []\n",
        "  for ents_i, ent in enumerate(doc.ents):\n",
        "    ent_start = ent.start\n",
        "    ent_end = ent.end\n",
        "    # List of all the children of the entity span tokens\n",
        "    subtree = list(ent.root.subtree)\n",
        "    search_start = subtree[0].i\n",
        "    search_end = subtree[-1].i + 1\n",
        "    # The search should be limited by previous and next entities\n",
        "    if ents_i > 0 and doc.ents[ents_i - 1].end > search_start:\n",
        "      search_start = doc.ents[ents_i - 1].end\n",
        "    if ents_i < (len(doc.ents) - 1) and doc.ents[ents_i + 1].start < search_end:\n",
        "      search_end = doc.ents[ents_i + 1].start\n",
        "    # Extend the head\n",
        "    token = doc[search_start]\n",
        "    while token.i < ent_start:\n",
        "      compound_root = token\n",
        "      while compound_root.dep_ == 'compound' and not (ent_start <= compound_root.i < ent_end):\n",
        "        compound_root = compound_root.head\n",
        "      if ent_start <= compound_root.i < ent_end:\n",
        "        ent_start = token.i\n",
        "      token = token.nbor()\n",
        "    # Extend the tail\n",
        "    token = doc[search_end - 1]\n",
        "    while token.i >= ent_end:\n",
        "      compound_root = token\n",
        "      while compound_root.dep_ == 'compound' and not (ent_start <= compound_root.i < ent_end):\n",
        "        compound_root = compound_root.head\n",
        "      if ent_start <= compound_root.i < ent_end:\n",
        "        ent_end = token.i + 1\n",
        "      token = token.nbor(-1)\n",
        "    # Add the expanded entity to the list\n",
        "    entity = Span(doc, ent_start, ent_end, label=ent.label_)\n",
        "    entities.append(entity)\n",
        "  # Set the extended entities\n",
        "  doc.set_ents(entities)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tpKTz-CB-t7"
      },
      "source": [
        "We can directy apply the postprocessing to the docs object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EudFK_mzJ_1s"
      },
      "source": [
        "# Application of the post-processing step\n",
        "for doc in docs:\n",
        "  expand_entities(doc)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSY01zuCtf_F"
      },
      "source": [
        "### Part 3.1: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSXVOvA4CImE"
      },
      "source": [
        "Results are worse. For instance, \"Shimon Peres\" is extended to \"minister Shimon Peres\" since minister has a compound relationship with Shimon, and that's clearly not the correct identification of the PERSON named entity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "NUI2ZAOLGSWb",
        "outputId": "1bf3d87d-ba5d-483a-a549-ca27f7eafd50"
      },
      "source": [
        "# Evaluation of the results\n",
        "NE_ref, NE_pred = token_entities(docs)\n",
        "print(classification_report(NE_ref, NE_pred))\n",
        "print('='*80)\n",
        "\n",
        "NE_ref, NE_pred = chunk_entities(docs)\n",
        "results = conll.evaluate(NE_ref, NE_pred)\n",
        "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
        "pd_tbl.round(decimals=3)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.78      0.69      0.73      4074\n",
            "      B-MISC       0.13      0.61      0.21      2000\n",
            "       B-ORG       0.48      0.31      0.38      3636\n",
            "       B-PER       0.68      0.56      0.61      3900\n",
            "       I-LOC       0.46      0.60      0.52       593\n",
            "      I-MISC       0.05      0.30      0.09       696\n",
            "       I-ORG       0.46      0.57      0.51      2061\n",
            "       I-PER       0.68      0.82      0.75      2785\n",
            "           O       0.95      0.86      0.90     96735\n",
            "\n",
            "    accuracy                           0.81    116480\n",
            "   macro avg       0.52      0.59      0.52    116480\n",
            "weighted avg       0.89      0.81      0.84    116480\n",
            "\n",
            "================================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>p</th>\n",
              "      <th>r</th>\n",
              "      <th>f</th>\n",
              "      <th>s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ORG</th>\n",
              "      <td>0.432</td>\n",
              "      <td>0.282</td>\n",
              "      <td>0.341</td>\n",
              "      <td>3636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MISC</th>\n",
              "      <td>0.122</td>\n",
              "      <td>0.576</td>\n",
              "      <td>0.201</td>\n",
              "      <td>2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PER</th>\n",
              "      <td>0.661</td>\n",
              "      <td>0.544</td>\n",
              "      <td>0.596</td>\n",
              "      <td>3900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LOC</th>\n",
              "      <td>0.771</td>\n",
              "      <td>0.679</td>\n",
              "      <td>0.722</td>\n",
              "      <td>4074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>total</th>\n",
              "      <td>0.379</td>\n",
              "      <td>0.519</td>\n",
              "      <td>0.438</td>\n",
              "      <td>13610</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           p      r      f      s\n",
              "ORG    0.432  0.282  0.341   3636\n",
              "MISC   0.122  0.576  0.201   2000\n",
              "PER    0.661  0.544  0.596   3900\n",
              "LOC    0.771  0.679  0.722   4074\n",
              "total  0.379  0.519  0.438  13610"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}